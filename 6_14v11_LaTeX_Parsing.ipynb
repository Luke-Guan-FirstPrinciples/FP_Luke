{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "--> NER the bottle neck? --> Chat with Nawar?\n",
        "\n",
        "--> A comparison between Semantic Scholar, Grobid, LaTexWalker. (provide pros & cons & feasibility of each approach & next step)\n",
        "\n",
        "**TODO** --> Run this script on a bigger scale, see the success failure rate S3 pipeline\n",
        "\n",
        "What I did:\n",
        "manually randomly selected more paper --> processed --> found more issues (sometimes I don't know, sometimes I know '}' ) --> improve algorithm (including Regex) --> increase duarbility\n",
        "\n",
        "\n",
        "v7 --> v8. added more PDFs\n",
        "\n",
        "v8 --> v9 reduced errors from 6 down to 3\n",
        "\n",
        "V10  --> addressing \"/newblock\" + more robust @ detection\n",
        "\n",
        "resolved:\n",
        "\n",
        "for example it would falsely match this line:\n",
        "\n",
        "\\g@addto@macro\\@parboxrestore{\\lineskiplimit\\normallineskip}\n",
        "\n",
        "or this line (false):\n",
        "\n",
        "\\providecommand \\@ifxundefined [1]{\\@ifx{#1\\undefined}\n",
        "\n"
      ],
      "metadata": {
        "id": "BjraFzTs_aPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Style Detected         | What to Do                                         |\n",
        "| ---------------------- | -------------------------------------------------- |\n",
        "| `\\bibitem`             | Already handled via `parse_bibitem_block()`        |\n",
        "| `\\bibliography{}`      | Locate and parse `.bib` files using `bibtexparser` |\n",
        "| `\\bibliographystyle{}` | Optional: record it (not used in parsing directly) |\n",
        "\n"
      ],
      "metadata": {
        "id": "sFzQDh-BRMWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch 1:\n",
        "\n",
        "## Not sure:\n",
        "1912.00001\n",
        "\n",
        "\n",
        "## poor format:\n",
        "File: 1912.04804.tex\n",
        "\n",
        "\n",
        "## failed expansion:\n",
        "File: 1912.04801.tex\n",
        "\n",
        "\n",
        "# Batch 2.  - June 13th\n",
        "\n",
        "\n",
        "\n",
        "@ARTICLE{Zevin2019,\n",
        "\tauthor = {{Zevin}, M. and {Kremer}, K. and et al.},\n",
        "     year = 2019,\n",
        "}\n",
        "\n",
        "\n",
        "basically a huge dump of info\n",
        "\n",
        "why 1911.00018 works but 1911.00003 & 1911.00010 doesn't work?\n",
        "\n",
        "**Ans:  Slaccitation -  missing a closing \"}\". How????**\n",
        "\n",
        "**TODO**\n",
        "\n",
        "1911.00008 looks like 1912.04804\n",
        "\n",
        "  \\bibfield  {author} {\\bibinfo {author} {\\bibfnamefont {A.}~\\bibnamefont\n",
        "  {Polkovnikov}}, \\bibinfo {author} {\\bibfnamefont {K.}~\\bibnamefont\n",
        "\n",
        "\n",
        "\\bibliographystyle{hyperplain}\n",
        "\\begin{thebibliography}{10}\n",
        "\n",
        "\n",
        "1911.00006 why ?\n",
        "\n",
        "\\bibitem{Agol11}\n",
        "Ian Agol.\n",
        "\\newblock Ideal triangulations of pseudo-{A}nosov mapping tori.\n",
        "\\newblock In {\\em Topology and Geometry in Dimension Three: Triangulations,\n",
        "  Invariants, and Geometric Structures (Proceedings of the Jacofest\n",
        "  conference)}, volume 560 of {\\em AMS Contemporary Mathematics}, pages 1--18,\n",
        "  2011,  \\href{http://arxiv.org/abs/1008.1606v2}{{arXiv:1008.1606v2}}.\n",
        "\n",
        "\n",
        "ASK Nawar --> NER\n",
        "Attempt to find a dataset?\n"
      ],
      "metadata": {
        "id": "jmbHf9Pftz_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 1911.00003\n",
        "\n",
        "\n",
        "\n",
        "> @article{Banks:1975gq,\n",
        "\tAuthor = {Banks, Tom and Susskind, Leonard and Kogut, John B.},\n",
        "\tDate-Added = {2019-04-08 14:22:07 +0200},\n",
        "\tDate-Modified = {2019-04-11 17:39:37 +0200},\n",
        "\tDoi = {10.1103/PhysRevD.13.1043},\n",
        "\tJournal = {Phys. Rev. D},\n",
        "\tPages = {1043},\n",
        "\tReportnumber = {CLNS-318},\n",
        "\tSlaccitation = {Title = {{Strong Coupling Calculations of Lattice Gauge Theories: (1+1)-Dimensional Exercises}},\n",
        "\tVolume = {13},\n",
        "\tYear = {1976},\n",
        "\tBdsk-Url-1 = {http://dx.doi.org/10.1103/PhysRevD.13.1043}}\n",
        "\n"
      ],
      "metadata": {
        "id": "1sb7scMd-Kq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QVDnNBA7ROEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO:\n",
        "\n",
        "Adding Href handling\n",
        "\n",
        "Compare with Grobib"
      ],
      "metadata": {
        "id": "k_0x8XcLSNO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OOCwme-_WpID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/parsed_references.txt"
      ],
      "metadata": {
        "id": "8c8Efq0qO2fG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Latex-papers"
      ],
      "metadata": {
        "id": "OcvtQYS8jO-H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Luke-Guan-FirstPrinciples/Latex-papers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYwu_XOcTs4X",
        "outputId": "6696d97c-c692-4b01-9a0d-61f5e44b66ec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Latex-papers'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 63 (delta 6), reused 39 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (63/63), 1.35 MiB | 5.78 MiB/s, done.\n",
            "Resolving deltas: 100% (6/6), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TB4rNpYup9uj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bibtexparser pylatexenc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu7DPEsOJ10j",
        "outputId": "8a116c24-f587-467d-eb26-fd702bfecf16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bibtexparser\n",
            "  Downloading bibtexparser-1.4.3.tar.gz (55 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pylatexenc\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from bibtexparser) (3.2.3)\n",
            "Building wheels for collected packages: bibtexparser, pylatexenc\n",
            "  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bibtexparser: filename=bibtexparser-1.4.3-py3-none-any.whl size=43549 sha256=db50966894e903064e17666bbda973e633bc3d6356525e498c8848ff9312a461\n",
            "  Stored in directory: /root/.cache/pip/wheels/16/fb/76/306387739cf9d53b1c39b0c8aadbbb17dc05f256756d8fd915\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=fb60d602e518643da0b24bfde6a41348fc2e91ba28febd6ad20491468a7d6626\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/7a/33/9fdd892f784ed4afda62b685ae3703adf4c91aa0f524c28f03\n",
            "Successfully built bibtexparser pylatexenc\n",
            "Installing collected packages: pylatexenc, bibtexparser\n",
            "Successfully installed bibtexparser-1.4.3 pylatexenc-2.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "HiS11PC9KuXL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SijNVMzFTywP",
        "outputId": "a7c0f228-c1f9-426d-d2fe-4a3dac27341a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import bibtexparser\n",
        "from pylatexenc.latexwalker import LatexWalker, LatexCharsNode, LatexMacroNode, LatexGroupNode, LatexEnvironmentNode\n",
        "\n",
        "\n",
        "def clean_latex_with_latexwalker(latex_string):\n",
        "    walker = LatexWalker(latex_string)\n",
        "    try:\n",
        "        nodelist, pos, _ = walker.get_latex_nodes(pos=0)\n",
        "    except Exception as e:\n",
        "        print(f\"❌ LatexWalker failed: {e}\")\n",
        "        return latex_string\n",
        "\n",
        "    def _extract_text(nodes):\n",
        "        text = ''\n",
        "        for node in nodes:\n",
        "            if isinstance(node, LatexCharsNode):\n",
        "                text += node.chars\n",
        "            elif isinstance(node, LatexGroupNode):\n",
        "                text += _extract_text(node.nodelist)\n",
        "            elif isinstance(node, LatexMacroNode):\n",
        "                if node.macroname == 'href' and len(node.nodeargs) == 2:\n",
        "                    # \\href{url}{text} → just keep {text}\n",
        "                    text += _extract_text(node.nodeargs[1].nodelist)\n",
        "                elif node.macroname in ['em', 'textbf', 'textit']:\n",
        "                    if node.nodeargs:\n",
        "                        text += _extract_text(node.nodeargs[0].nodelist)\n",
        "                elif node.macroname == 'newblock':\n",
        "                    # Treat as a sentence separator\n",
        "                    text += '. '\n",
        "                else:\n",
        "                    # Unknown macro — ignore or log\n",
        "                    pass\n",
        "        return text\n",
        "\n",
        "    result = re.sub(r'\\s+', ' ', _extract_text(nodelist)).strip()\n",
        "    return result\n",
        "\n",
        "\n",
        "def parse_bibitem_block_robust(tex_content):\n",
        "    print(\"🔍 DEBUG: Starting parse_bibitem_block_robust\")\n",
        "\n",
        "    # Trim everything outside thebibliography\n",
        "    bib_section = re.search(r'\\\\begin\\{thebibliography\\}.*?\\\\end\\{thebibliography\\}', tex_content, flags=re.DOTALL)\n",
        "    if not bib_section:\n",
        "        print(\"❌ DEBUG: No thebibliography section found\")\n",
        "        return []\n",
        "\n",
        "    content = bib_section.group(0)\n",
        "    # print(f\"✅ DEBUG: Found bibliography section, length: {len(content)}\")\n",
        "    # print(f\"📝 DEBUG: First 200 chars of content: {repr(content[:200])}\")\n",
        "\n",
        "    # Use a more robust pattern to find all \\bibitem entries\n",
        "    # This pattern captures everything from \\bibitem to the next \\bibitem or \\end{thebibliography}\n",
        "    pattern = r'\\\\bibitem(?:\\[[^\\]]*\\])?\\{([^}]+)\\}(.*?)(?=\\\\bibitem|\\\\end\\{thebibliography\\})'\n",
        "    matches = re.findall(pattern, content, flags=re.DOTALL)\n",
        "\n",
        "    # print(f\"🔍 DEBUG: Found {len(matches)} regex matches\")\n",
        "    # for i, (key, body) in enumerate(matches):\n",
        "    #     print(f\"📋 DEBUG: Match {i+1}: key='{key}', body_length={len(body)}\")\n",
        "    #     print(f\"📋 DEBUG: Match {i+1} body preview: {repr(body[:100])}\")\n",
        "\n",
        "    parsed = []\n",
        "    for i, (key, body) in enumerate(matches):\n",
        "        key = key.strip()\n",
        "        body = body.strip()\n",
        "\n",
        "        # print(f\"🔧 DEBUG: Processing match {i+1}: key='{key}'\")\n",
        "\n",
        "        # # Skip if body is empty\n",
        "        # if not body:\n",
        "        #     print(f\"⚠️ DEBUG: Skipping match {i+1} - empty body\")\n",
        "        #     continue\n",
        "\n",
        "        # Clean the body - remove \\newblock and other LaTeX commands\n",
        "        # print(f\"🧹 DEBUG: Cleaning body for '{key}'\")\n",
        "        cleaned = clean_latex_with_latexwalker(body)\n",
        "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "        print(f\"✨ DEBUG: Cleaned result for '{key}': {repr(cleaned[:100])}\")\n",
        "\n",
        "        if cleaned:  # Only add if we have actual content\n",
        "            parsed.append({\n",
        "                'key': key,\n",
        "                'raw': cleaned\n",
        "            })\n",
        "            # print(f\"✅ DEBUG: Added entry '{key}' to results\")\n",
        "        else:\n",
        "            print(f\"⚠️ DEBUG: Skipping '{key}' - cleaned content is empty\")\n",
        "\n",
        "    # print(f\"🎉 DEBUG: Final result: {len(parsed)} entries parsed\")\n",
        "    return parsed\n",
        "\n",
        "\n",
        "# V9\n",
        "# ✅ Improvements Plan\n",
        "# Use a manual parser fallback: When a BibTeX entry is incomplete, bibtexparser will likely fail. So:\n",
        "\n",
        "# Detect and try to salvage broken entries by bracket-counting.\n",
        "\n",
        "# Allow parsing of partial or malformed entries as best-effort, rather than discarding them entirely.\n",
        "\n",
        "# More flexible regex: Match from @type{ up to the next @ or end of file, and try to balance braces manually to find complete entries.\n",
        "\n",
        "# Warn and keep raw string if it can't parse.\n",
        "\n",
        "# 🧠 Strategy: Balanced Braces Scanning\n",
        "# We can do this by scanning line-by-line from each @...{ and tracking {/} balance, collecting until balance reaches zero (or EOF).\n",
        "\n",
        "def extract_embedded_bibtex_entries(tex_content):\n",
        "    print(\"extract_embedded_bibtex_entries\")\n",
        "    \"\"\"\n",
        "    Extract BibTeX entries from a LaTeX string robustly,\n",
        "    even when entries are malformed or have missing closing brackets.\n",
        "    \"\"\"\n",
        "    entries = []\n",
        "    lines = tex_content.splitlines()\n",
        "    i = 0\n",
        "    while i < len(lines):\n",
        "        line = lines[i].strip()\n",
        "        if line.lower().startswith('@'):\n",
        "            entry_lines = [line]\n",
        "            brace_depth = line.count('{') - line.count('}')\n",
        "            i += 1\n",
        "            while i < len(lines) and brace_depth > 0:\n",
        "                entry_lines.append(lines[i])\n",
        "                brace_depth += lines[i].count('{') - lines[i].count('}')\n",
        "                i += 1\n",
        "            entry_text = '\\n'.join(entry_lines).strip()\n",
        "\n",
        "            # Quick cleanup: ensure it ends with }\n",
        "            if not entry_text.endswith('}'):\n",
        "                print(f\"⚠️ Incomplete entry (missing closing brace):\\n{entry_text[:100]}...\\nTrying to recover...\")\n",
        "\n",
        "                # Try to auto-close it\n",
        "                entry_text += '\\n}'\n",
        "\n",
        "            try:\n",
        "                bib_database = bibtexparser.loads(entry_text)\n",
        "                if bib_database.entries:\n",
        "                    entries.extend(bib_database.entries)\n",
        "                else:\n",
        "                    print(f\"⚠️ Could not parse entry: {entry_text[:100]}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Failed to parse BibTeX entry:\\n{entry_text[:100]}...\\nError: {e}\")\n",
        "                # Optionally, store raw fallback\n",
        "                entries.append({'ID': 'unknown', 'raw': entry_text})\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    print(f\"✅ Parsed {len(entries)} BibTeX entries (including fallbacks)\")\n",
        "    return entries\n",
        "\n",
        "import re\n",
        "\n",
        "def parse_latex_references(tex_path):\n",
        "    print(f\"📂 DEBUG: Opening file: {tex_path}\")\n",
        "\n",
        "    with open(tex_path, 'r', encoding='utf-8') as f:\n",
        "        tex_content = f.read()\n",
        "\n",
        "\n",
        "    # Original v9.     bibtex_entry_regex = re.compile(r'@\\w+\\s*{')\n",
        "\n",
        "    # --- Robust BibTeX Entry Detection ---\n",
        "    bibtex_entry_regex = re.compile(\n",
        "        r'^[ \\t]*@(?:article|book|inproceedings|misc|phdthesis|techreport|incollection)\\s*\\{',\n",
        "        flags=re.IGNORECASE | re.MULTILINE\n",
        "    )\n",
        "\n",
        "    for lineno, line in enumerate(tex_content.splitlines(), start=1):\n",
        "        if bibtex_entry_regex.match(line):\n",
        "            print(f\"🐛 DEBUG: BibTeX entry detected on line {lineno}: {line.strip()}\")\n",
        "            break  # Remove this break if you want to list all BibTeX matches\n",
        "\n",
        "    if bibtex_entry_regex.search(tex_content):\n",
        "        print(\"🔍 DEBUG: Found embedded BibTeX entries\")\n",
        "        return extract_embedded_bibtex_entries(tex_content)\n",
        "\n",
        "    # --- thebibliography Environment Detection ---\n",
        "    for lineno, line in enumerate(tex_content.splitlines(), start=1):\n",
        "        if '\\\\begin{thebibliography}' in line:\n",
        "            print(f\"📚 DEBUG: Found \\\\begin{{thebibliography}} on line {lineno}: {line.strip()}\")\n",
        "            break\n",
        "\n",
        "    if '\\\\begin{thebibliography}' in tex_content:\n",
        "        print(\"🔍 DEBUG: Found thebibliography environment\")\n",
        "        return parse_bibitem_block_robust(tex_content)\n",
        "\n",
        "    # --- No Recognized Format Found ---\n",
        "    print(\"❌ DEBUG: No bibliography format detected\")\n",
        "    return []\n",
        "\n"
      ],
      "metadata": {
        "id": "GkqNZMf5ODL0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pformat\n",
        "\n",
        "# --- Batch Processing Script ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    folder_path = '/content/Latex-papers/batch2'  # Folder containing .tex files\n",
        "    output_path = '/content/parsed_references.txt'\n",
        "\n",
        "    with open(output_path, 'w', encoding='utf-8') as out_file:\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith('.tex'):\n",
        "                full_path = os.path.join(folder_path, filename)\n",
        "                print(f\"\\n📄 Processing: {filename}\")\n",
        "                try:\n",
        "                    references = parse_latex_references(full_path)\n",
        "                    out_file.write(f\"File: {filename}\\n\")\n",
        "                    out_file.write(pformat(references, indent=2))\n",
        "                    out_file.write(\"\\n\\n\" + \"=\"*80 + \"\\n\\n\")\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"❌ Failed to parse {filename}: {e}\"\n",
        "                    print(error_msg)\n",
        "                    out_file.write(f\"{error_msg}\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8Xg5BIjE0AJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b564a272-73e4-45b0-c6db-56710dcaac78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📄 Processing: 1911.00016.tex\n",
            "📂 DEBUG: Opening file: /content/Latex-papers/batch2/1911.00016.tex\n",
            "🐛 DEBUG: BibTeX entry detected on line 1166: @ARTICLE{York2000AJ....120.1579Y,\n",
            "🔍 DEBUG: Found embedded BibTeX entries\n",
            "extract_embedded_bibtex_entries\n",
            "❌ Failed to parse BibTeX entry:\n",
            "@PHDTHESIS{1990PhDT.......263C,\n",
            "   author = {{Carini}, M.~T.},\n",
            "    title = \"{The time scales of the ...\n",
            "Error: 'january'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I asked Gemini:\n",
        "\n",
        "\n",
        "\n",
        "Show more\n",
        "\n",
        "Conversation with Gemini\n",
        "Are the following references the same style:\n",
        "\n",
        "\n",
        "\n",
        "    'raw': 'authorD.Acosta etal. (collaborationCDF Collaboration), '\n",
        "\n",
        "           'journalPhys. Rev. Lett. volume95, pages022003 (year2005).'},\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    'raw': 'Aldous, D.J. (1985). Exchangeability and related topics. In cole '\n",
        "\n",
        "           \"d't de Probabilits de Saint-Flour XIII—1983, pages 1198. \"\n",
        "\n",
        "           'Springer.'},\n",
        "\n",
        "\n",
        "\n",
        "    'raw': 'Berg, A.R., Jordn, T.: Algorithms for graph rigidity and scene '\n",
        "\n",
        "           'analysis. In: Proc. 11th European Symposium on Algorithms (ESA '\n",
        "\n",
        "           \"'03), LNCS, vol. 2832, pp. 7889. (2003)\"},\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    'raw': \"A. D. Aleksandrov and V. V. Strel'tsov, The isoperimetric problem \"\n",
        "\n",
        "           'and estimates of the length of a curve on a surface, Proc. Steklov '\n",
        "\n",
        "           'Inst. Math. 76 (1965), 67-80; English transi., Proc. Steklov Inst '\n",
        "\n",
        "           'Math. (1967), 81-99.'},\n",
        "\n",
        "\n",
        "\n",
        "    'raw': 'Coffman, V., Kundu, J., Wootters, W.K.: Distributed entanglement. '\n",
        "\n",
        "           'Phys. Rev. A 61, 052306 (2000)'},\n",
        "\n",
        "\n",
        "\n",
        "    'raw': 'Bakos, G., Noyes, R.W., Kovcs, G., Stanek, K.Z., Sasselov, D.D., '\n",
        "\n",
        "           'Domsa, I. 2004, , 116, 266'},\n"
      ],
      "metadata": {
        "id": "zdlOHFyYsQY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The references provided are **not** all in the same style.\n",
        "\n",
        "\n",
        "\n",
        "* **Reference 1 (Acosta et al.):**\n",
        "    * **Style indicators:** Author last name, initials, collaboration name, journal abbreviation, volume, pages, year in parentheses. This looks like a common style for physics journals (e.g., APS style).\n",
        "    * `authorD.Acosta etal. (collaborationCDF Collaboration), journalPhys. Rev. Lett. volume95, pages022003 (year2005).`\n",
        "\n",
        "* **Reference 2 (Aldous):**\n",
        "    * **Style indicators:** Author last name, initials, year in parentheses after author, title of work, \"In\" followed by edited volume title, pages, publisher. This is often seen in conference proceedings or edited books, sometimes resembling a variant of APA or Chicago.\n",
        "    * `Aldous, D.J. (1985). Exchangeability and related topics. In cole d't de Probabilits de Saint-Flour XIII—1983, pages 1198. Springer.`\n",
        "\n",
        "* **Reference 3 (Berg, Jordán):**\n",
        "    * **Style indicators:** Author last name, initials (sometimes separated by a colon), title, \"In:\" followed by conference name/proceedings, LNCS (Lecture Notes in Computer Science) volume, page range, year in parentheses. This is very common in computer science conference proceedings.\n",
        "    * `Berg, A.R., Jordn, T.: Algorithms for graph rigidity and scene analysis. In: Proc. 11th European Symposium on Algorithms (ESA '03), LNCS, vol. 2832, pp. 7889. (2003)`\n",
        "\n",
        "* **Reference 4 (Aleksandrov and Strel'tsov):**\n",
        "    * **Style indicators:** Initials first, then last name for authors, title of work, journal name (abbreviated), volume, year, page range, and then a separate entry for English translation. This particular style with the \"English transi.\" separated out is less common but indicates a specific format.\n",
        "    * `A. D. Aleksandrov and V. V. Strel'tsov, The isoperimetric problem and estimates of the length of a curve on a surface, Proc. Steklov Inst. Math. 76 (1965), 67-80; English transi., Proc. Steklov Inst Math. (1967), 81-99.`\n",
        "\n",
        "* **Reference 5 (Coffman, Kundu, Wootters):**\n",
        "    * **Style indicators:** Author last name, initials (sometimes with no comma or space between last name and initial), title, journal abbreviation, volume, page number, year in parentheses. Similar to the first one, likely a physics journal style.\n",
        "    * `Coffman, V., Kundu, J., Wootters, W.K.: Distributed entanglement. Phys. Rev. A 61, 052306 (2000)`\n",
        "\n",
        "* **Reference 6 (Bakos et al.):**\n",
        "    * **Style indicators:** Last name, initials, then year, then some numbers that might correspond to volume and page (though the formatting is unusual with double commas and no explicit journal title). This looks incomplete or highly idiosyncratic.\n",
        "    * `Bakos, G., Noyes, R.W., Kovcs, G., Stanek, K.Z., Sasselov, D.D., Domsa, I. 2004, , 116, 266`\n",
        "\n",
        "**In summary:** There are variations in author-year placement, punctuation, abbreviations, and the inclusion of elements like \"In:\" or \"Proc.\" which clearly indicate different citation styles are being used across these examples."
      ],
      "metadata": {
        "id": "JZP7gcgNsFwO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rL_qwQ6DsEk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract --> Yes\n",
        "\n",
        "Where do we go from here?\n",
        "\n",
        "Classify + Recognize --> Not sure"
      ],
      "metadata": {
        "id": "9dI6YNP-tK9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some failed cases:\n",
        "\n",
        "1902.00005\n",
        "\n",
        "\\bibliographystyle{IEEEtran}\n",
        "@ARTICLE{Clarke,\n",
        "  author = {R. H. Clarke},\n",
        "  title = {A statistical theory of mobile-radio reception},\n",
        "  journal = {Bell Syst. Tech. J.},\n",
        "  year = {1968},\n",
        "  volume = {47},\n",
        "  pages = {957-1000},\n",
        "  publisher = {Bell Syst.}\n",
        "}\n",
        "\n",
        "@inproceedings{rezende_stochastic_2014,\n",
        "\taddress = {Beijing, China},\n",
        "\tseries = {{ICML}'14},\n",
        "\ttitle = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},\n",
        "\turl = {http://dl.acm.org/citation.cfm?id=3044805.3045035},\n",
        "\tabstract = {We},\n",
        "\turldate = {2018-10-05},\n",
        "\tbooktitle = {Proceedings of the 31st {International} {Conference} on {International} {Conference} on {Machine} {Learning} - {Volume} 32},\n",
        "\tpublisher = {JMLR.org},\n",
        "\tauthor = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},\n",
        "\tyear = {2014},\n",
        "\tpages = {II--1278--II--1286}\n",
        "}\n",
        "\n",
        "@ARTICLE{Zemen,\n",
        "  author = {T. Zemen and C. F. Mecklenbrauker and F. Kaltenberger and B. H. Fleury},\n",
        "  title = {Minimum-Energy Band-Limited Predictor With Dynamic Subspace Selection for Time-Variant Flat-Fading Channels},\n",
        "  journal = {IEEE Trans. on Signal Process.},\n",
        "  year = {2007},\n",
        "  volume = {55},\n",
        "  pages = {4534-4548}\n",
        "}\n",
        "\n",
        "\n",
        "\\bibitem\n",
        "\n",
        "BibTeX-style references"
      ],
      "metadata": {
        "id": "uVs6K3jQPfuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above has been resolved\n",
        "\n",
        "\n",
        "1912.04801 -- Citation Section is missing\n",
        "\n",
        "1912.04804 -- Weird formatting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "For the next step, NER, LLM is only way that I could think of\n"
      ],
      "metadata": {
        "id": "MW5RYMP_-FZ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normal\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\\bibliographystyle{h-physrev-qian}\n",
        "\\begin{thebibliography}{10}\n",
        "\n",
        "\n",
        "\\bibitem{qcd-phase-fluctuations-02}\n",
        "M.~A. Stephanov,\n",
        "\\newblock Phys. Rev. Lett. {\\bf 102} (2009) 032301, arXiv:0809.3450.\n",
        "```\n",
        "\n",
        "Failed\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\\providecommand{\\href}[2]{#2}\\begin{thebibliography}{10}\n",
        "\n",
        "\\bibitem{strominger}\n",
        "A.~Strominger, ``Superstrings with torsion,'' {\\em Nucl. Phys.} {\\bf B274}\n",
        "  (1986)\n",
        "253.\n",
        "\n",
        "\n",
        "\\bibitem{gauntlett-pakis}\n",
        "J.~P. Gauntlett and S.~Pakis, ``{The geometry of $D=11$ Killing spinors},''\n",
        "  {\\em JHEP} {\\bf 04} (2003) 039,\n",
        "\\href{http://arXiv.org/abs/hep-th/0212008}{{\\tt hep-th/0212008}}.\n",
        "\n",
        "\\bibitem{ward}\n",
        "R.~Ward, ``{Einstein--Weyl spaces and ${\\rm SU}(\\infty)$ Toda fields},'' {\\em\n",
        "  Classical and Quantum Gravity} {\\bf 7} (1990), no.~4, L95.\n",
        "\n",
        "\\end{thebibliography}\n",
        "\n",
        "\\bibliographystyle{at}\n",
        "\n",
        "```\n",
        "\n",
        "The \\href command is used to create a clickable link to an online resource, in this case, a pre-print on arXiv.org.\n",
        "\n",
        "\n",
        "Some entries have extra lines between items (like 253. or the \\href{} link).\n",
        "\n",
        "\n",
        "\n",
        "--> is this good enough, requires more testing\n",
        "\\\\bibitem(?:\\[[^\\]]*\\])?\\{([^\\}]+)\\}(.*?)(?=(\\\\bibitem|\\Z))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YhmxVx1AAm4w"
      }
    }
  ]
}